# LRU Cache [#146](https://leetcode.com/problems/lru-cache/description/) üíæ‚ú®


## üß© Problem statement

Implement an LRU (least recently used) cache that supports two operations in O(1) time on average:

* `get(key)` ‚Äî return the value of the key if it exists in the cache, otherwise return `-1`. Accessing an entry marks it as most recently used.
* `put(key, value)` ‚Äî insert or update the value. If the cache exceeds its capacity, evict the least recently used item.

We want an in-memory structure that evicts the least recently used entry when capacity is exceeded.

### üóÇÔ∏è Least Recently Used (LRU)

Imagine your backpack can only hold **three snacks** at a time.

You keep grabbing new snacks throughout the day, but there‚Äôs a rule:

#### **If you want to put a new snack in and the backpack is full, you must throw out the snack you haven‚Äôt touched for the longest time.**

That‚Äôs an **LRU Cache** : *Least Recently Used*.

* Each ‚Äúsnack‚Äù = a piece of data the cache stores
* The backpack = the limited-size cache
* ‚ÄúHaven‚Äôt touched in a while‚Äù = data that hasn‚Äôt been accessed recently
* Throwing it out to make room = eviction policy

So an **LRU cache** is like a backpack with a strict teacher inside saying:

> ‚ÄúYou‚Äôre not eating those stale crackers. Out they go! Make room for the new cookies you actually want!‚Äù

It keeps whatever you‚Äôve used **most recently**, and gently tosses the stuff you‚Äôve forgotten about.


<details>
  <summary><p>üìñ Wikipedia Article</p></summary>
Discards least recently used items first. This algorithm requires keeping track of what was used and when, which is cumbersome. It requires **"age bits"** for [cache lines](https://en.wikipedia.org/wiki/CPU_cache#Cache_entries), and tracks the `least recently used` cache line based on these age bits. When a cache line is used, the age of the other cache lines changes.

**LRU** is a family of [caching algorithms](https://en.wikipedia.org/wiki/Page_replacement_algorithm#Variants_on_LRU), that includes `2Q` by *Theodore Johnson* and *Dennis Shasha*[[7](https://en.wikipedia.org/wiki/Cache_replacement_policies#cite_note-7)] and `LRU/K` by *Pat O'Neil*, *Betty O'Neil* and *Gerhard Weikum*.[[8](https://en.wikipedia.org/wiki/Cache_replacement_policies#cite_note-8)]

The access sequence for the example is **A B C D E D F**:

![image](https://upload.wikimedia.org/wikipedia/commons/4/43/Mruexample.png)


When **A B C D** is installed in the blocks with sequence numbers (increment 1 for each new access) and **E** is accessed, it is a [miss](https://en.wikipedia.org/wiki/CPU_cache#Cache_miss) and must be installed in a block. With the LRU algorithm, E will replace A because A has the lowest rank (A(0)). In the next-to-last step, D is accessed and the sequence number is updated. F is then accessed, replacing B ‚Äì which had the lowest rank, (B(1)).

</details>


---

### Intuition

We need two things:

1. Fast lookup of a key ‚Üí use a hash map (unordered_map).
2. Fast update of "recency" ordering (move a node to the front or end when accessed) ‚Üí use a doubly linked list.

Put together:

* Hash map: `key -> pointer to linked list node` for O(1) lookup.
* Doubly linked list: store nodes in LRU ‚Üí MRU order (or vice versa). Removing and inserting nodes is O(1).

So `get`:

* lookup in map, if found move node to MRU position and return value.

`put`:

* if key exists: update value and move node to MRU.
* else: create new node at MRU. If size exceeds capacity, remove LRU node from list and map.


### Brute force (what you'd avoid)

Store items in a vector. On `get`, find the key by linear scan `O(n)`. On `put`, update/evict by scanning to find LRU. Result is `O(n)` per op, not acceptable.

We want `O(1)` average time per operation. That‚Äôs where hashmap + doubly linked list shines.


### The approach we use

* Maintain two sentinel nodes `head` and `tail` (also called `oldest` and `latest`) so insertion and deletion code is simpler and boundary-safe.
* Store nodes between `head` and `tail`. `head->next` is the LRU; `tail->prev` is the MRU.
* The hash map maps `key` to the corresponding `Node*`.
* When using a node, remove it from list and insert it before `tail` (MRU position).
* When capacity exceeded, remove `head->next` (LRU) and erase from map.


### Clean C++ implementation

```cpp
#include <bits/stdc++.h>
using namespace std;

class Node {
public:
    int key;
    int val;
    Node* prev;
    Node* next;
    Node(int k, int v) : key(k), val(v), prev(nullptr), next(nullptr) {}
};

class LRUCache {
private:
    int capacity;
    unordered_map<int, Node*> mp; // key -> node*
    Node* head; // sentinel: oldest / LRU side
    Node* tail; // sentinel: newest / MRU side

    // remove node from the doubly linked list
    void removeNode(Node* node) {
        if (!node) return;
        Node* p = node->prev;
        Node* n = node->next;
        if (p) p->next = n;
        if (n) n->prev = p;
        node->prev = node->next = nullptr;
    }

    // insert node right before tail (make it MRU)
    void insertBeforeTail(Node* node) {
        // tail and its prev should always exist because we initialize sentinels
        Node* prevNode = tail->prev;
        prevNode->next = node;
        node->prev = prevNode;
        node->next = tail;
        tail->prev = node;
    }

    // move an existing node to MRU position
    void moveToMRU(Node* node) {
        if (!node) return;
        if (node->next == tail) return; // already MRU
        removeNode(node);
        insertBeforeTail(node);
    }

    // evict least recently used node (head->next)
    void evictLRU() {
        Node* lru = head->next;
        if (lru == tail) return; // nothing to evict
        removeNode(lru);
        mp.erase(lru->key);
        delete lru;
    }

public:
    LRUCache(int cap) : capacity(cap), head(new Node(0,0)), tail(new Node(0,0)) {
        head->next = tail;
        tail->prev = head;
        if (capacity < 0) capacity = 0;
        mp.reserve(capacity > 0 ? capacity * 2 : 4);
    }

    ~LRUCache() {
        // delete all nodes (including sentinels)
        Node* cur = head;
        while (cur) {
            Node* nxt = cur->next;
            delete cur;
            cur = nxt;
        }
    }

    int get(int key) {
        auto it = mp.find(key);
        if (it == mp.end()) return -1;
        Node* node = it->second;
        moveToMRU(node);
        return node->val;
    }

    void put(int key, int value) {
        auto it = mp.find(key);
        if (it != mp.end()) {
            Node* node = it->second;
            node->val = value;
            moveToMRU(node);
            return;
        }

        // new node
        Node* node = new Node(key, value);
        mp[key] = node;
        insertBeforeTail(node);

        if ((int)mp.size() > capacity) {
            evictLRU();
        }
    }
};
```

> Notes:
>
> * We use sentinel `head` and `tail` so insert/remove code never needs to check for nullptr edges.
> * `mp.reserve(...)` is optional but can reduce rehashing overhead.
> * The memory cleanup in destructor traverses the list and deletes everything.

---

### Step-by-step example

Capacity = 2

Operations:

1. `put(1, 10)`

   * cache: (LRU) 1 ‚Üí (MRU)
2. `put(2, 20)`

   * cache: 1, 2 (2 is MRU)
3. `get(1)` ‚Üí returns 10

   * mark 1 MRU ‚Üí cache now: 2 (LRU), 1 (MRU)
4. `put(3, 30)`

   * capacity exceeded ‚Üí evict LRU (2)
   * cache: 1, 3
5. `get(2)` ‚Üí -1 (evicted)
6. `get(3)` ‚Üí 30, mark 3 MRU


### Test cases to run

* Basic sequence: put/get as above.
* Update existing key: `put(1,1)`, `put(1,10)` then `get(1)` should return 10 and be MRU.
* Capacity 1: ensure evictions happen immediately when adding second element.
* Capacity 0: either always evict or treat as no storage. Our implementation will accept capacity 0 and evict immediately.
* Repeated gets: `get` should move items to MRU even if value unchanged.
* Large input stress test: many random put/get operations to validate O(1) behavior.


### Complexity

* Time complexity:

  * `get` is O(1) average. (unordered_map lookup + O(1) linked list ops)
  * `put` is O(1) average. (map ops + constant list ops + possible eviction)
* Space complexity:

  * O(capacity) for the map + list nodes.


### Tips, tricks, and pitfalls

* **Store indices vs nodes:** we must store pointers/iterators to list nodes in the map. Storing values alone won‚Äôt allow O(1) repositioning.
* **Use sentinel nodes:** simplifies insert/delete and avoids null checks.
* **Remember to delete nodes** if you `new` them, to avoid memory leaks. The destructor above cleans up the list.
* **Edge cases:** capacity = 0. Decide desired semantics. Our code will accept it and evict immediately.
* **Thread safety:** not provided. You‚Äôd need locks if using from multiple threads.
* **Alternative simpler implementation:** use `std::list<pair<int,int>>` and `unordered_map<int, list::iterator>`. That saves you writing custom Node and manual deletion. Example below.


### Alternative implementation using `std::list` (short & idiomatic)

```cpp
#include <bits/stdc++.h>
using namespace std;

class LRUCache {
private:
    int capacity;
    list<pair<int,int>> items;                 // front = MRU, back = LRU
    unordered_map<int, list<pair<int,int>>::iterator> mp;

public:
    LRUCache(int cap) : capacity(cap) {}

    int get(int key) {
        auto it = mp.find(key);
        if (it == mp.end()) return -1;
        // move the accessed item to front (MRU)
        items.splice(items.begin(), items, it->second);
        return it->second->second;
    }

    void put(int key, int value) {
        auto it = mp.find(key);
        if (it != mp.end()) {
            // update and move to front
            it->second->second = value;
            items.splice(items.begin(), items, it->second);
            return;
        }
        // insert new item at front
        items.emplace_front(key, value);
        mp[key] = items.begin();

        if ((int)mp.size() > capacity) {
            // evict LRU from back
            auto last = items.end();
            --last;
            mp.erase(last->first);
            items.pop_back();
        }
    }
};
```

This version is shorter and easy to reason about. `splice` is O(1`), so it keeps the same complexity.


### Variations and related problems

* All sorts of cache eviction policies: MRU, LFU (least frequently used), FIFO.
* Design caches with time-based expiry or weighted eviction.
* Multi-level caches and write-back vs write-through policies in systems design.


### FAQs

**Q: Why not use just `unordered_map`?**
> A: The map gives O(1) lookup, but doesn't give ordering for LRU eviction or O(1) repositioning. We need the list to update recency in O(1).

**Q: Can we use `map` instead of `unordered_map`?**
> A: `map` gives O(log n) operations, which is slower. `unordered_map` gives O(1) average.

**Q: Is the list+map implementation optimal?**
> A: Yes, for the LRU problem it's the standard optimal approach: O(1) average for both operations with O(capacity) space.

**Q: Should I use raw pointers or `std::list`?**
> A: `std::list` is safer, shorter and less error prone. Raw nodes can be slightly faster but require careful memory management.

